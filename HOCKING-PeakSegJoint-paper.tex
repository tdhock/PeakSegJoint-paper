\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}

\title{PeakSegJoint: supervised detection of the same peaks jointly
  across several ChIP-seq samples}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
  Joint peak detection is a central problem when comparing samples in
  genomic data analysis, and current algorithms for this task are
  unsupervised and mostly effective for a single data type and pattern
  (e.g. H3K4me3 data with a sharp peak pattern). We propose
  PeakSegJoint, a new constrained multi-sample maximum likelihood
  segmentation model. To select the number of peaks in the
  segmentation, we propose a new supervised penalty learning model. To
  infer the parameters of these two models, we propose to use a
  discrete optimization heuristic for the segmentation, and convex
  optimization for the penalty learning. We show that this method
  achieves state-of-the-art peak detection, and results in
  interpretable peaks that occur in exactly the same positions across
  samples.
\end{abstract}

\section{Introduction}

\subsection{Peak detection in ChIP-seq data}

Chromatin immunoprecipitation sequencing (ChIP-seq) is a biological
experiment for genome-wide profiling of histone modifications and
transcription factor binding sites, with many experimental and
computational steps \citep{practical}. Briefly, each experiment yields
a set of sequence reads which are aligned to a reference genome, and
then the number of aligned reads are counted at each genomic position
(Figure~\ref{fig:good-bad}). These data can be interpreted using
one of the many available peak detection algorithms
\citep{evaluation2010, rye2010manually, chip-seq-bench}, which each
essentially work as a binary classifier for each genomic position. The
positive class is enriched (peaks) and the negative class is
background noise. Importantly, peaks and background occur in long
contiguous segments across the genome.

TODO: more intro about joint peak detection.

In supervised peak detection, there are $n$ labeled genomic regions,
and each $i\in\{1, \dots, n\}$ has a set of labels $L_i$ (``noPeaks,''
``peaks,'' etc. as in Figures~TODO). These labels define a non-convex
annotation error function
\begin{equation}
  \label{eq:error}
  E[c(\mathbf y_i),  L_i] =
  \text{FP}[c(\mathbf y_i), L_i] +
  \text{FN}[c(\mathbf y_i), L_i]
\end{equation}
which counts the number of false positive (FP) and false negative (FN)
regions, so it takes values in the non-negative integers. The goal of
learning is to find a peak caller~$c$ with minimal error on some test
profiles:
\begin{equation}
  \label{eq:min_error}
  \minimize_c \sum_{i\in\text{test}} E[c(\mathbf y_i),  L_i].
\end{equation}
More specifically, we suppose that the training data and the test data
exhibit the same pattern type.
\section{Related work}

\citet{hierarchical-joint} describe an HMM. TODO describe.

\citet{jmosaics} is another model. TODO describe.

\citet{JAMM} describe JAMM a joint method for analyzing replicates. It
says that it and DFilter are both ``universal'' for broad and narrow
peaks. They used TF bindings sites as positive controls. They say that
it is better to analyze replicates independantly than to pool
replicates.

\citet{PePr} describe PePr a method for identifying consistent or
differential peaks across replicates. They applied it to TF data sets,
and used visual inspection to test (but not train).

\begin{figure}[b!]
  \centering
  \includegraphics[width=\textwidth]{figure-PeakSegJoint}
  \caption{Blue line segments show the sequence of PeakSegJoint models
    for $S=3$ samples and $p\in\{0, 1, 2, 3\}$ peaks.}
  \label{fig:PeakSegJoint}
\end{figure}

\section{Models}

\subsection{PeakSeg: finding the best $0,\dots,p_{\text{max}}$ peaks
  in a single sample}

Given a single sample profile $\mathbf z\in\ZZ_+^B$ of aligned reads
across $B$ bases, and a maximum number of peaks $p_{\text{max}}\leq
B$, the PeakSeg model with $p\in\{0, \dots, p_{\text{max}}\}$ peaks is
\begin{align}
  \label{PeakSeg}
  \mathbf{\tilde m}^p(\mathbf z)  =
    \argmin_{\mathbf m\in\RR^{B}} &\ \ 
    \text{PoissonLoss}(\mathbf m, \mathbf z) 
    \tag{\textbf{PeakSeg}}
\\
    \text{such that} &\ \  \Peaks(\mathbf m)=p,  \\
     \forall j\in\{1, \dots, B\}, &\ \ P_j(\mathbf m) \in\{0, 1\},
\end{align}
where the Poisson loss function is
\begin{equation}\label{eq:rho}
  \text{PoissonLoss}(\mathbf m, \mathbf y)= \sum_{j=1}^B m_j - y_j \log m_j,
\end{equation} 
the model complexity is the number of peaks
\begin{equation}
  \Peaks(\mathbf m)=(\Segments(\mathbf m)-1)/2,
\end{equation}
which is a function of the number of segments
\begin{equation}
  \Segments(\mathbf m)=1+\sum_{j=2}^B I(m_j \neq m_{j-1}),
\end{equation}
and the peak indicator at base $j$ is
\begin{equation}
  \label{eq:peaks}
  P_j(\mathbf m) = \sum_{k=2}^j \sign( m_{k} - m_{k-1} ),
\end{equation}
with $P_1(\mathbf m)=0$ by convention.

\subsection{PeakSegJoint: finding the best common peak in $0,\dots, S$
  samples}

For $S$ sample profiles $\mathbf z_1, \dots, \mathbf z_S\in\ZZ_+^B$
defined on the same $B$ bases, we stack the vectors into a matrix
$\mathbf Z\in\ZZ_+^{B \times S}$. Consider the following model which
allows each sample to have either 0 or 1 peaks, with a total of
$p\in\{0, \dots, S\}$ peaks:
\begin{align}
  \label{Unconstrained}
  \mathbf{\tilde M}^p(\mathbf Z)  =
  \argmin_{\mathbf M\in\RR^{B\times S}} &\ \ 
  \sum_{s=1}^S 
  \text{PoissonLoss}(\mathbf m_s, \mathbf z_s) 
  \tag{\textbf{Unconstrained}}
  \\
  \text{such that} &\ p = \sum_{s=1}^S \Peaks(\mathbf m_s)
  \label{total_Peaks}
  \\
  &\ \forall s\in\{1, \dots, S\},\, 
  \Peaks(\mathbf m_s)\in\{0, 1\},  
  \label{zero_or_one}
  \\
  &\ \forall s\in\{1, \dots, S\},\, 
  \forall j\in\{1, \dots, B\},\, P_j(\mathbf m_s) \in\{0, 1\}.
  \label{up_down}
\end{align}
This optimization problem is \ref{Unconstrained} in the sense that the
peaks are not required to be in the exact same locations in each of
the $S$ samples. The constraint (\ref{up_down}) is the same constraint
as in \ref{PeakSeg}, which here requires the segment mean $\mathbf
m_s$ of each sample to have alternating changes (up, down, up, down
and not up, up, down). The constraint on the number of peaks per
sample (\ref{zero_or_one}) means that each sample may have either zero
or one peak. Finally, the overall constraint (\ref{total_Peaks}) means
that there is a total of $p$ samples each with exactly 1 peak (not
necessarily with the same start/end positions across samples).

It is easy to see that the \ref{Unconstrained} solution in $p$ peaks
can be written in terms of the \ref{PeakSeg} solutions in 0 or 1 peaks:
\begin{equation}
  \label{eq:unconstrained_PeakSeg}
  \mathbf{\tilde M}^p = \left[
    \begin{array}{ccc}
      \mathbf{\tilde m}^{p_1}(\mathbf z_1) & 
      \cdots &
      \mathbf{\tilde m}^{p_S}(\mathbf z_S) 
    \end{array}
  \right],
\end{equation}
for some $p_1,\dots, p_S\in\{0, 1\}$ with $p=\sum_{i=s}^S p_s$.

The PeakSegJoint model with $p\in\{0, \dots, S\}$ peaks is defined by
introducing one more constraint (\ref{joint_constraint}):
\begin{align}
  \label{PeakSegJoint}
  \mathbf{\hat M}^p(\mathbf Z)  =
  \argmin_{\mathbf M\in\RR^{B\times S}} &\ \ 
  \sum_{s=1}^S 
  \text{PoissonLoss}(\mathbf m_s, \mathbf z_s) 
  \tag{\textbf{PeakSegJoint}}
  \\
  \text{such that} &\ p = \sum_{s=1}^S \Peaks(\mathbf m_s)
  \nonumber
  \\
  &\ \forall s\in\{1, \dots, S\},\, 
  \Peaks(\mathbf m_s)\in\{0, 1\},  
  \nonumber
  \\
  &\ \forall s\in\{1, \dots, S\},\,
  \forall j\in\{1, \dots, B\},\, P_j(\mathbf m_s) \in\{0, 1\},
  \nonumber
  \\
  &\ \forall s_1\neq s_2\mid
  \nonumber
  \Peaks(\mathbf m_{s_1})=\Peaks(\mathbf  m_{s_2})=1,\,
  \forall j\in\{1, \dots, B\},\\
  &\ \ P_j(\mathbf m_{s_1}) = P_j(\mathbf m_{s_2}).
  \label{joint_constraint}
\end{align}
TODO: explain.

\subsection{Supervised penalty learning}

In the last section we considered data $\mathbf Z\in\ZZ_+^{B\times S}$
for $S$ samples in a single genomic region with $B$ bases. Now assume
that we have data $\mathbf Z_1,\dots, \mathbf Z_n\in\ZZ_+^{B\times S}$
for $n$ genomic regions, along with annotated region labels
$L_1,\dots, L_n$. 

For each genomic region $i\in\{1,\dots,n\}$ we can compute a sequence
of \ref{PeakSegJoint} models $\mathbf{\hat M}^0(\mathbf Z_i),\dots,
\mathbf{\hat M}^S(\mathbf Z_i)$, but how will we predict which of
these $S+1$ models will be best?
% in terms of the test annotated region labels $L_i$? 
This is the segmentation model selection problem, which we propose to
solve via supervised learning of a penalty function.

First, for a positive penalty constant $\lambda\in\RR_+$, we define
the optimal number of peaks as
\begin{equation}
  \label{eq:optimal_segments}
  p^*(\lambda, \mathbf Z) =
  \argmin_{p\in\{0, \dots, S\}}
  p \lambda + 
  \text{PoissonLoss}\left[
    \mathbf{\hat M}^p(\mathbf Z),
    \mathbf Z
  \right].
\end{equation}
Also suppose that we can compute $d$-dimensional sample-specific
feature vectors $\mathbf x\in\RR^d$ and stack them to obtain feature
matrices $\mathbf X_1,\dots, \mathbf X_n\in\RR^{d\times S}$. We will
learn a function $f:\RR^{d\times S}\rightarrow\RR$ that predicts
region-specific penalty values $f(\mathbf X_i) = \log \lambda_i\in\RR$
for each genomic region $i$. In particular we will learn a weight
vector $\mathbf w\in\RR^d$ in a linear function $f_{\mathbf w}(\mathbf X) =
\mathbf w^\intercal \mathbf X \mathbf 1_S$, where $\mathbf 1_S$ is a
vector of $S$ ones.

For supervision we use the annotated region labels $L_i$ to compute
the annotation error (\ref{eq:error}) and a target interval $\mathbf
y_i = (
    \underline y_i, \overline y_i
)$ of penalty values. We used the algorithm of
\citet{HOCKING-penalties} to compute the exact target interval.
% The
% target interval consists of the lower $\underline y_i$ and upper
% $\overline y_i$ limits of the largest region of the $\log \lambda$
% penalty space that has minimum error (\ref{eq:error}). 
Briefly,
a predicted penalty in the target interval $f(\mathbf
X_i)\in\mathbf y_i$ implies that the
PeakSegJoint model with $p^*\left[\exp f(\mathbf X_i), \mathbf
  Z_i\right]$ peaks achieves the minimum number of incorrect labels
$L_i$, among all $S+1$ PeakSegJoint models for genomic region $i$.
%  $\exp f(\mathbf X_i)\in \argmin_\lambda
% E\left[ \mathbf P\big( \mathbf{\hat M}^{p^*(\lambda, \mathbf
%     Z)}(\mathbf Z) \big), L_i\right]$ (TODO: should we actually show
% this equation or just use words?).

A target interval $\mathbf y$ is used with the squared hinge loss
$\phi(x)=(x-1)^2 I(x\leq 1)$ to define the surrogate loss
\begin{equation}
  \label{eq:surrogate_loss}
  \ell\left[
    \mathbf y,\,
    \log \hat \lambda
    \right]
    =
    \phi\big[
      \log\hat\lambda - \underline y
    \big]
    +
    \phi\big[
    \overline y - \log\hat\lambda
    \big],
\end{equation}
for a predicted penalty value $\hat \lambda\in\RR_+$. For a weight parameter
$\mathbf w\in\RR^d$, the average surrogate loss over the entire data
set is
\begin{equation}
  \label{eq:average_surrogate}
  \mathcal L(\mathbf w) =
  \frac 1 n
  \sum_{i=1}^n
  \ell\left[
    \mathbf y_i,\,
     f_{\mathbf w}( \mathbf X_i )
    \right].
\end{equation}
Finally, we add a 1-norm penalty to regularize and encourage a sparse
weight vector, thus obtaining the following convex supervised penalty
learning problem:
\begin{equation}
  \label{argmin_w}
  \mathbf{\hat w}^\gamma = 
  \argmin_{\mathbf w\in\RR^d}
  \mathcal L(\mathbf w) + \gamma ||\mathbf w||_1.
\end{equation}

To predict on test data $\mathbf Z$ with features $\mathbf
X$, we compute the the predicted penalty $\hat \lambda = \exp
f_{\mathbf{\hat w}}(\mathbf X)$, the predicted number of
peaks $\hat p = p^*(\hat \lambda, \mathbf Z)$, and finally the
predicted model $\mathbf{\hat M}^{\hat p}(\mathbf Z)$.

\section{Algorithms}

\subsection{Heuristic discrete optimization for joint segmentation}

TODO: make figure that explains bin pyramid.

\subsection{Convex optimization for supervised penalty learning}

The convex supervised learning problem (\ref{argmin_w}) can be
computed using gradient-based methods such as FISTA, a Fast Iterative
Shinkage-Thresholding Algorithm \citep{fista}. To apply FISTA, we need
to compute the gradient of the smooth average surrogate loss,
\begin{equation}
  \label{eq:average_gradient}
  \nabla \mathcal L(\mathbf w) = 
  \frac 1 n
  \sum_{i=1}^n 
  \nabla \ell \left[
    \mathbf y_i,\,
    f_{\mathbf w}(  \mathbf X_i )
  \right],
\end{equation}
where the gradient of one observation $i$ is
\begin{equation}
  \label{eq:one_gradient}
  \nabla \ell \left[
    \mathbf y_i,\,
    f_{\mathbf w}( \mathbf X_i )
  \right]
  =
  \mathbf X_i \mathbf 1_S
  \left[
    \phi'\big(
    f_{\mathbf w}( \mathbf X_i ) - \underline y_i
    \big)
    -
    \phi'\big(
    \overline y_i - f_{\mathbf w}( \mathbf X_i )
    \big)
  \right],
\end{equation}
and the derivative of the squared hinge loss is $\phi'(x)=2(x-1)I(x\leq 1)$.

We used the Lipschitz constant and stopping criterion suggested by
\citet{HOCKING-penalties}.

% An optimality condition for (\ref{argmin_w}) is 
% \begin{equation}
%   \label{eq:optimality}
%   \nabla \mathcal L(\mathbf w) \in \gamma \partial ||\mathbf w||_1.
% \end{equation}

\section{Results}

\section{Discussion}

\section{Conclusions}

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
